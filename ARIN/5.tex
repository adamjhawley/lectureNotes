\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}

\author{Adam Hawley}
\title{Lecture 5: Informed Search}

\begin{document}

\maketitle

\section{Informed vs. Uninformed Search}
Search algorithms differ by how they pick which nodes to expand and when.

\subsection{Uninformed Search Algorithms}
In making this decision, these look only at the structure of the search tree and not at the statess inside the nodes
These are often also called \textbf{blind} searcj algorithms.
We have seen 5 of these in \href{3.pdf}{lecture 3}.

\subsection{Informed Search Algorithms}
In making this decision these look at the states inside the nodes.
Also sometimes called \textbf{heuristic} search algorithms.
These will be covered in this lecture.

\section{Informed Search Algorithms in Detail}
\subsection{Algorithm \romannumeral1: Best-First Search}
Main idea: Use an \textit{evaluation function} $f(n)$ for each node $n$.
This function estimates the \textit{desirability} of expanding that node.
And then you expand the \textit{most desirable} unexpanded node.

Implementation:
Order the nodes in the fringe $L$ in decreasing order of desirability.
Then always choose nodes from the front of $L$.

The evaluation $f(n)$ will be $h(n)$, the function which gives the value of the heurisitc e.g the straight line distance.
Greedy best-first search expands the node that appears to be closest to the goal.

\begin{itemize}
	\item Completeness:
		There are a variety of cases where completeness is lost e.g looping.
	\item Time complexity:
		$O(b^m)$, but a good heuristic can give dramatic improvement
	\item Space comlpexity:
		$O(b^m)$ --- keeps all nodes in memory
	\item Optimal:
		No since sometimes it doesn't find a solution at all.
\end{itemize}

\subsection{A* Search}
Main idea:
avoid expanding paths that are already expensive.
Here the evaluation function $f(n) = g(n) + h(n)$ where 
\begin{itemize}
	\item $g(n)$ is the cost so far to reach $n$.
	\item $h(n)$ is the estimated cost from $n$ to the nearest goal state.
	\item $f(n)$ is the estimated total cost of path through $n$ to a goal state.
\end{itemize}

\textbf{Note:} $f(n) = g(n)$ is used by uniform cost search and $f(n) = h(n)$ is used by greedy best-first search.

\section{Admissable Heuristics}
A heuristic is admissable if for every node $n$, $h(n)\le h^*(n)$, where $h^*(n)$ is the true cost to reach the nearest goal state from $n$.
Therefore, an admissable heuristc nevver overestimates the cost to reach the goal i.e it is optimistic.

\subsection{Dominance of Heuristics}
If $h_2(n) \ge h_1(n)$ for all $n$ and both $h1$ and $h2$ are admissable, then $h_2$ is said to ``dominate'' $h_1$.

\smallskip
\textbf{Theorem:} If $h_2$ dominates $h_1$ then A* with $h_1$ expands every node expanded by A* with $h_2$ and possibly more.

\subsection{How to Obtain Admissable Heuristics?}
A problem with fewer restrictions on the actions is called a \textit{relaxed problem}.
The cost of an optimal solution to a relaxed problem is an admissable heuristic for the orignal problem.

\subsection{Consistent Heuristics}
A heuristic is consistent if for every node $n$, every successor $n'$ of $n$ generated by any action $a$, $h(n) \le c(n,a,n') + h(n')$.

If $h$ is consistent, we have:
\begin{align*}
	f(n') &= g(n') + h(n') \\
		&= g(n) + c(n,a,n') + h(n') \\
		&\ge g(n) + h(n) \\
		&\ge f(n)
\end{align*}
i.e, $f(n)$ is non-decreasing along any path, that is, $f(n)$ is \textbf{monotonic}.

\smallskip
\textbf{Theorem:} If a heuristic is consistent then it is admissable.

\smallskip
\textbf{Theorem:} If $h(n)$ is consistent, A* can be implemented more efficiently --- no need ever to reevaluate the cost of already expanded nodes).

\section{Tree Search vs. Graph Search}
\begin{itemize}
	\item Graph search solves the problem with revisting states
	\item Efficient probkem representation \& tree search may be enough.
	\item DFS/IDS using tree search could check the current path to eliminate looops
	\item Graph search stores nodes corresponding to all visited states.
\end{itemize}

\end{document}
