% Created 2019-03-13 Wed 22:57
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Adam Hawley}
\date{\today}
\title{ML Section 2: Reinforcement Learning}
\hypersetup{
 pdfauthor={Adam Hawley},
 pdftitle={ML Section 2: Reinforcement Learning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.2)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Psychology \& Engineering of Reinforcement Learning}
\label{sec:orga1c31ec}
See lecture for history.

\section{Agent Paradigm}
\label{sec:org3e4211e}
A common setup for the agent paradigm of RL involves having an agent which carries out \textbf{actions} on an \textbf{environment} through the use of \textbf{effectors}.
These actions are based on the \textbf{reward signal} and \textbf{states} from the environment which are read using \textbf{sensors}.

\section{RL vs. Supervised Learning}
\label{sec:org57ef0bc}
What makes RL different?
\begin{itemize}
\item No supervisor feedback: learning from reward signal only which is often delayed, not instantaneous.
\item Agent influences the selection of training experience.
\item Sequential aspect of decision making.
\end{itemize}

\section{The Markov Decision Process}
\label{sec:org62007d6}
The Markov decision process is used to model the environment.

A (single-agent) MDP is a tuple (S,A,p,R) where:
\begin{description}
\item[{S}] A set of states
\item[{A}] A set of actions
\item[{p: S x A x S \(\rightarrow\) [0,1]}] Specifies the transition probabilities between states.
\item[{R: S x A \(\rightarrow\) R}] Specifies the reward for each state-action pair.
\end{description}

Note: deterministic transition function \(\delta: S \times A \implies S\).

\section{Agent State vs. Environment State}
\label{sec:org20a716c}
It is very important to recognise the difference between these two.
Agents have their own representation of the environment state.
Agents may have \textbf{partial observability} so they are not able to observe everything in the environment.
This may be benefitial as they may want to focus in on relevant parts of the environment.
Ideally only the information necessary to make an optimal decision is contained in the agent state.

\section{Markov States}
\label{sec:org17b8afe}
A state S\textsubscript{t} is Markov if and only if S\textsubscript{t} contains all relevant information to determine the next state.

Note: any state can be made into a Markov state by incorporating the complete history.

\section{RL Output}
\label{sec:org434f86b}
\textbf{Goal}: learn an \emph{optimal policy} \(\pi\): S \(\rightarrow\) A

Evaluation of policy via discounted cumulative reward:
\begin{equation}
V^{\pi}(S_t) = \sum\limits_{i \le 0} \gamma^i r_{t+1}
\end{equation}
where:
\begin{itemize}
\item \(0\le \gamma < 1\) is a discount factor (\(\gamma\) = 0 means that only immediate reward is considered).
\item r\textsubscript{t+1} is the reward at time t+1 determined by performing actions specified by policy \(\pi\).
\end{itemize}
The goal previously mentioned can now be redefined as: Agent learns policy \(\pi\) that maximises \(V^{\pi}(s)\) for all states s.
So the optimal policy \(\pi\)\textsuperscript{*} = argmax\textsubscript{\(\pi\)} V\textsuperscript{\(\pi\)}(S) for all S.
\textbf{Notation}: \(V^{\pi^*}(S) = V^*(S)\).

The optimal action in state s is the action a that maximises the sum of the immediate reward r(s,a) plus the value V\textsuperscript{*} of the immedaite successor state, discounted by \(\gamma\):
\begin{equation}
\pi*(S) = argmax_a[r(s,a) + \gamma V^*(\delta(s,a))]
\end{equation}
If functions r and \(\delta\) are known then the agent can acquire \(\pi\)\textsuperscript{*} by computing V\textsuperscript{*}.
\end{document}
